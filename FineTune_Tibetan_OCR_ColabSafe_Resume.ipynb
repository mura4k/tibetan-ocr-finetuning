{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install albumentations pyctcdecode pyewts botok huggingface_hub natsort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "\n",
        "# Download dataset\n",
        "dataset_path = snapshot_download(repo_id=\"BDRC/Karmapa8\", repo_type=\"dataset\", cache_dir=\"Datasets\")\n",
        "with ZipFile(f\"{dataset_path}/data.zip\", 'r') as zip:\n",
        "    zip.extractall(f\"{dataset_path}/Dataset\")\n",
        "\n",
        "# Download model\n",
        "model_path = snapshot_download(repo_id=\"BDRC/Woodblock\", repo_type=\"model\", cache_dir=\"Models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from BudaOCR.Modules import EasterNetwork, OCRTrainer, WylieEncoder\n",
        "from BudaOCR.Utils import create_dir, shuffle_data, build_data_paths, read_ctc_model_config\n",
        "import time\n",
        "import os\n",
        "\n",
        "# === Model config ===\n",
        "model_path = snapshot_download(repo_id=\"BDRC/BigUCHAN_v1\", repo_type=\"model\", cache_dir=\"Models\")\n",
        "model_config = f\"{model_path}/config.json\"\n",
        "ctc_config = read_ctc_model_config(model_config)\n",
        "\n",
        "label_encoder = WylieEncoder(ctc_config.charset)\n",
        "num_classes = label_encoder.num_classes()\n",
        "image_width = ctc_config.input_width\n",
        "image_height = ctc_config.input_height\n",
        "\n",
        "# === Load dataset ===\n",
        "image_paths, label_paths = build_data_paths(dataset_path)\n",
        "print(f\"Images: {len(image_paths)}, Labels: {len(label_paths)}\")\n",
        "image_paths, label_paths = shuffle_data(image_paths, label_paths)\n",
        "\n",
        "\n",
        "\n",
        "# === Initialize network ===\n",
        "network = EasterNetwork(num_classes=num_classes, image_width=image_width, image_height=image_height, mean_pooling=True)\n",
        "network.fine_tune(f\"{model_path}/BigUCHAN_E_v1.pth\")\n",
        "\n",
        "# === Trainer Setup ===\n",
        "output_dir = \"Output\"\n",
        "create_dir(output_dir)\n",
        "\n",
        "ocr_trainer = OCRTrainer(\n",
        "    network=network,\n",
        "    label_encoder=label_encoder,\n",
        "    workers=4,\n",
        "    image_width=image_width,\n",
        "    image_height=image_height,\n",
        "    batch_size=32,\n",
        "    output_dir=output_dir,\n",
        "    preload_labels=True\n",
        ")\n",
        "\n",
        "ocr_trainer.init(image_paths, label_paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === Resume from disk ===\n",
        "def get_latest_checkpoint(output_dir: str):\n",
        "    ckpts = glob(os.path.join(output_dir, \"checkpoint_epoch_*.pth\"))\n",
        "    if not ckpts:\n",
        "        return None, 0\n",
        "    ckpts.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
        "    latest_ckpt = ckpts[-1]\n",
        "    epoch_num = int(latest_ckpt.split(\"_\")[-1].split(\".\")[0])\n",
        "    return latest_ckpt, epoch_num\n",
        "\n",
        "latest_ckpt, start_epoch = get_latest_checkpoint(ocr_trainer.output_dir)\n",
        "\n",
        "if latest_ckpt:\n",
        "    print(f\"üîÅ Resuming from checkpoint: {latest_ckpt} (epoch {start_epoch})\")\n",
        "    network.load_checkpoint(latest_ckpt)\n",
        "else:\n",
        "    print(\"üÜï Starting training from scratch.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop with checkpointing and logging to file\n",
        "total_epochs = 64\n",
        "log_file = os.path.join(ocr_trainer.output_dir, \"training_log.txt\")\n",
        "\n",
        "for epoch in range(start_epoch, total_epochs):\n",
        "    now = time.ctime()\n",
        "    print(f\"[{now}] üß™ Epoch {epoch+1}/{total_epochs}\")\n",
        "    with open(log_file, \"a\") as log:\n",
        "        log.write(f\"[{now}] Epoch {epoch+1}/{total_epochs}\\n\")\n",
        "\n",
        "    loss = network.train(ocr_trainer.train_loader)\n",
        "\n",
        "    with open(log_file, \"a\") as log:\n",
        "        log.write(f\"[{time.ctime()}] ‚úÖ Loss: {loss:.4f}\\n\")\n",
        "\n",
        "    # Save checkpoint every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_file = os.path.join(ocr_trainer.output_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
        "        torch.save(network.get_checkpoint(), ckpt_file)\n",
        "        print(f\"[{time.ctime()}] üíæ Saved checkpoint: {ckpt_file}\")\n",
        "        with open(log_file, \"a\") as log:\n",
        "            log.write(f\"[{time.ctime()}] üíæ Saved checkpoint: {ckpt_file}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate and export\n",
        "cer_scores = ocr_trainer.evaluate()\n",
        "cer_values = list(cer_scores.values())\n",
        "\n",
        "with open(os.path.join(ocr_trainer.output_dir, \"cer_scores.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for sample, value in cer_scores.items():\n",
        "        f.write(f\"{sample} - {value}\\n\")\n",
        "\n",
        "with open(os.path.join(ocr_trainer.output_dir, \"cer_summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(f\"Mean CER: {np.mean(cer_values)}\\n\")\n",
        "    f.write(f\"Max CER: {np.max(cer_values)}\\n\")\n",
        "    f.write(f\"Min CER: {np.min(cer_values)}\\n\")\n",
        "\n",
        "# Export ONNX\n",
        "network.export_onnx(out_dir=ocr_trainer.output_dir, model_name=\"OCRModel\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
